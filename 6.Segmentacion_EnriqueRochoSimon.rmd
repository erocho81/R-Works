---
title: 'Fundamentos de Data Science: PEC2 - Algoritmos de segmentación'
author: "Enrique Rocho Simon"
date: "Diciembre del 2022"
output:
  html_document:
    highlight: default
    number_sections: yes
    theme: cosmo
    toc: yes
    toc_depth: 2
    includes:
      in_header:
  pdf_document:
    highlight: zenburn
    toc: yes
  word_document: default
---

******

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


******
# Introducción
******

## Descripción de la PEC a realizar
La prueba está estructurada en 6 ejercicios teórico-prácticos que piden que se desarrolle la fase de preparación y estimación de un modelo utilizando un juego de datos.  

Deben responderse al menos 4 de los 6 ejercicios para poder superar la PEC. Para optar a la máxima nota tienen que responderse todos los ejercicios de forma correcta.  

## Criterios de evaluación
**Ejercicios teóricos**  
Todos los ejercicios deben ser presentados de forma razonada y clara. No se aceptará ninguna respuesta que no esté claramente justificada.  

**Ejercicios prácticos**  
Para todas las PEC es necesario documentar en cada ejercicio práctico qué se ha hecho y cómo se ha hecho.  

Pregunta  | Criterio de valoración| Peso
---- | ------------- | ----
1  | Se justifica la eleccion de las variables | 5%
1  | Se describen los principales estadísticos | 5%
2  | Se justifica el numero de clusters | 10%
2  | Se entrena el modelo kmeans | 10%
3  | Se describen todos los clusters | 5%
4  | Se entrenan los modelos | 10%
4  | Se muestran la tablas de relaciones entre modelos |10%
4  | Se comapran y analizan  | 5%
5  | Se realiza el primer gráfico  | 5%
5  | Se realiza el segundo gráfico  | 5%
5  | Se analizan y comentan los resultados obtenidos  | 10%
6  | Se define la Silueta | 10%
6  | Se representa gráficamente | 5%
6  | Se índica el número óptimo de clusters | 5%


## Formato y fecha de entega
El formato de entrega es: studentname-PECn.html  
Fecha de Entrega: 18/12/2022  
Se debe entregar la PEC en el buzón de entregas del aula  

******
# Base teórica
******

Esta práctica está basada en los puntos 3.3.1, 3.3.2 y 3.3.3 del material didáctico (Business Analytics) de la asignatura. En el punto 3.3.1 se explica el procedimiento de segmentación jerárquica, mientras en los puntos 3.3.2 y 3.3.3 se explican procedimientos de segmentación no jerárquica para la formación de grupos que, respecto a la información utilizada, sean homogéneos dentro de si mismos y heterogéneos entre unos y otros.   

A lo largo de la práctica se proponen una serie de representaciones gráficas que ayudan a la interpretación de los resultados, sin embargo, podéis insertar más visualizaciones de las propuestas o incluso más código del estrictamente exigido en los ejercicios, eso sí, siempre con el objetivo de completar y mejorar el estudio propuesto.  

En esta práctica importaremos los datos desde un fichero de texto .csv con los campos delimitados por ";". Dichos datos corresponden a la información sobre algunas características de una muestra de asegurados procedentes de una cartera de seguros de automóvil. Los datos han sido extraídos de una cartera de asegurados real, aunque para garantizar la confidencialidad de la información se ha seleccionado una muestra no representativa o sesgada de la realidad. 

******
# Objetivos e información disponible
******

El objetivo de esta segunda PEC se centra en la determinación de distintos perfiles de asegurados del automóvil. 

Las variables que se definen en la base de datos y sus contenidos son:

--poliza: Identificador de póliza

--Sexo: Sexo del cliente

--sri: Situación de riesgo o zona de circulación urbana o no urbana

--gdi: Contratada garantía de daños propios o no

--sin: Número de siniestros en el año analizado

--ant_comp: Antigüedad del cliente en la compañía (en años)

--ant_perm: Antigüedad del permiso de conducir del asegurado (en años)

--edad: Edad del asegurado (en años)

--ant_veh: Antigüedad del vehículo asegurado (en años).


******
# Apartados de la práctica
******
El código R que utilizaremos en la práctica se divide en apartados según las tareas que iremos realizando:  

* Directorio de trabajo

* Importación del fichero de datos .csv. 

* Manipulación y representación de las variables

* Normalización de atributos

* Agrupación jerárquica: Algoritmo aglomerativo

* Uso de la función hclust() para la aglomeración de elementos

* Representación gráfica, Dendograma

* Asignación de los clusters

* Representación de los cluster

* Representación gráfica de variables por cluster

* Agrupación no jerárquica: Algoritmo kmeans

* Uso de la función kmeans() para la formación de cluster (grupos o perfiles de individuos)

* Elección del número de clústers

* Asignación de los clusters

* Representación de los cluster

* Representación gráfica de los clústers
  
* Ejercicios PEC2: Análisis cluster


******
# Directorio de trabajo
******
Antes de pasar a la importación y análisis de los datos definimos un directorio de trabajo o carpeta donde tenéis guardado el fichero de datos. Recordad que si abrís el RStudio desde vuestro directorio de trabajo, pulsando sobre el fichero .RMD que se os proporciona como parte del enunciado, este paso no haría falta.

```{r,eval=TRUE,echo=TRUE}
#Cambiar el argumento de setwd() con vuestro directorio, recordad utilizad las barras /.

setwd("E:/Análisis de datos/Fundamentos Data Science/PEC2")

```

******
# Importación del fichero de datos .csv. Manipulación y representación de las variables.
******
En primer lugar leemos el fichero de datos con extensión .csv que contiene la información de las 8.088 pólizas analizadas y mostramos su cabecera.
```{r,eval=TRUE,echo=TRUE}
# Lectura de datos

Cartera<-read.table("E:/Análisis de datos/Fundamentos Data Science/PEC2/Datos_analisis_clusters.csv",head=TRUE,sep=";")
head(Cartera)
```
A continuación describimos su contenido con la función summary() y con algunos gráficos. Observamos que para las variables cuantitativas la función summary() proporciona una serie de estadísticos descriptivos relacionados con la posición de la variable (media, mediana, máximo, mínimo,...). Sin embargo, para las variables cualitativas el resultado muestra las frecuencias absolutas (número de casos) de las categorías de las variables.

```{r,eval=TRUE,echo=TRUE}
summary(Cartera)
```

Realizamos algunas representaciones gráficas para describir la base de datos Cartera, utilizamos las herramientas gráficas adecuadas para cada tipo de variable: Cualitativa o Cuantitativa. Recordad que, antes de realizar cualquier análisis, es imprescindible estudiar el comportamiento univariante y bivariante de las variables.

```{r,eval=TRUE,echo=TRUE}
plot(Cartera[c("ant_comp","ant_perm")], xlab="Fidelidad", ylab="Experiencia") 
title(main="Nube de puntos original", col.main="blue", font.main=1)

freq<-table(Cartera$sin)
freq
barplot(freq,xlab="Número de siniestros", ylab="Frecuencia")
title(main="Número de siniestros", col.main="blue", font.main=1)

table(Cartera$Sexo,Cartera$sin)
prop.table(table(Cartera$Sexo,Cartera$sin))
barplot(prop.table(table(Cartera$Sexo,Cartera$sin)),col=c("darkblue","red"))
legend(5,0.8,c("Hombre","Mujer"),fill = c("darkblue","red"))
```

******
# Normalización de atributos
******
El objetivo es utilizar la información cuantitativa relacionada con la experiencia (edad y ant_perm), con la fidelidad (ant_comp), con el vehículo (ant_veh) y con la siniestralidad (sin) para segmentar a los asegurados. Para ello, en primer lugar, definimos la base de datos con las variables cuantitativas que utilizamos en la segmentación, el resto de variables pueden servir para caracterizar los grupos formados.

```{r,eval=TRUE,echo=TRUE}
clus<-Cartera[,c("sin","ant_comp","ant_perm","edad","ant_veh")]
```

La varianza de las variables (o su rango de valores) utilizadas en el análisis son distintas debido a que miden características diferentes de los individuos y de su vehículo. Por ejemplo, entre las variables utilizadas en el cluster hay algunas que miden el número de años y otra que mide el número de siniestro, es decir, las escalas son muy distintas. Por tanto, antes de iniciar el proceso de segmentación es necesario normalizar los valores de las variables para eliminar el efecto de las distintas escalas de medida, esto equivale a restarles su media y dividirlas por su desviación estándar.

Para la normalización de las variables en la base de datos clus, en primer lugar copiamos su contenido en clus_norm:

```{r,eval=TRUE,echo=TRUE}
clus_norm<-clus
```

Remplazamos las columnas de clus_norm por las columnas de clus normalizadas:

```{r,eval=TRUE,echo=TRUE}
 clus_norm[,c("sin")] <- (clus$sin-mean(clus$sin))/sd(clus$sin)
 clus_norm[,c("ant_comp")] <- (clus$ant_comp-mean(clus$ant_comp))/sd(clus$ant_comp)
 clus_norm[,c("ant_perm")] <- (clus$ant_perm-mean(clus$ant_perm))/sd(clus$ant_perm)
 clus_norm[,c("edad")] <- (clus$edad-mean(clus$edad))/sd(clus$edad)
 clus_norm[,c("ant_veh")] <- (clus$ant_veh-mean(clus$ant_veh))/sd(clus$ant_veh)
```

Realizamos algunas representaciones gráficas para describir las variables normalizadas y comprobamos que la nube de puntos representada es igual a la original, lo único que cambia es la escala de los ejes.

```{r,eval=TRUE,echo=TRUE}
#Normalizadas
plot(clus_norm[c("ant_comp","ant_perm")], xlab="Fidelidad", ylab="Experiencia") 
title(main="Nube de puntos normalizados", col.main="blue", font.main=1)

#Originales
plot(clus[c("ant_comp","ant_perm")], xlab="Fidelidad", ylab="Experiencia") 
title(main="Nube de puntos originales", col.main="blue", font.main=1)
```

A PARTIR DE AHORA TRABAJAMOS CON LOS DATOS NORMALIZADOS.

******
# Agrupación jerárquica: Algoritmo aglomerativo
******

El algoritmo jerárquico es una técnica no supervisada de agrupación de elementos de forma iterativa. Se comienza con todos los elementos desagrupados y en cada iteración agrupa los dos elementos o grupos de elementos más próximos, utilizando un criterio de enlace, hasta que todos los elementos forman un único grupo.

******
## Uso de la función hclust() para la aglomeración de elementos  
******

Para poder agrupar los elementos es necesario establecer una métrica de distancia siendo la habitual la métrica euclídea. Dado que hay que establecer una distancia, las variables deben ser numéricas, por lo que si se quieren introducir variables categórica deberá crearse una variable indicador para cada una de las categorías.

```{r,eval=TRUE,echo=TRUE}
distances = dist(clus_norm, method = "euclidean")
```

Una vez considerada la distancia a utilizar y calculada la distancia entre los elementos a agrupar se utiliza el comando hclust de agrupación de elementos

```{r,eval=TRUE,echo=TRUE}
clus_norm_Jerarquico = hclust(distances, method = "ward.D")
```

******
## Representación gráfica, Dendograma   
******

Es importante señalar que no es necesario establecer de forma previa el número de conjuntos o clusters dado que el algoritmo culmina con la agrupación de todos los elementos en un único grupo. 

Esta agrupación se suele representar con un gráfico llamado dendograma que muestra las agrupaciones realizadas (líneas de agrupación) y la distancia entre los elementos o grupos de elementos (altura).

```{r,eval=TRUE,echo=TRUE}
plot(clus_norm_Jerarquico,main="dendograma",xlab="elementos",ylab="distancias")
```

Sobre el propio dendograma se pueden representar los grupos que se obtendrían al separar los elementos en un número k predefinido de grupos.

```{r,eval=TRUE,echo=TRUE}
plot(clus_norm_Jerarquico,main="dendograma",xlab="elementos",ylab="distancias")

rect.hclust(clus_norm_Jerarquico, k=2, border="yellow")
rect.hclust(clus_norm_Jerarquico, k=3, border="blue")
rect.hclust(clus_norm_Jerarquico, k=4, border="green")
rect.hclust(clus_norm_Jerarquico, k=6, border="red")
rect.hclust(clus_norm_Jerarquico, k=10, border="cyan")
```

******
## Asignación de los clusters   
******

Para mantener el conjunto de datos original lo primero que hacemos es crear una copia del conjunto de datos

```{r,eval=TRUE,echo=TRUE}
clus_jerarquico=clus

```

Una vez elegido el número de clusters, se puede asignar a cada elemento el cluster asignado. En este caso vamos a elegir 4 clusters.

```{r,eval=TRUE,echo=TRUE}
NumCluster=4

clus_jerarquico$clusterJerar= cutree(clus_norm_Jerarquico, k = NumCluster)
head(clus_jerarquico)
```

******
## Representación de los cluster  
******
Para poder interpretar los clusters es habitual representarlos mediante el valor medio de las variables en cada cluster.

```{r,eval=TRUE,echo=TRUE}
aggregate(.~clusterJerar,FUN=mean, data=clus_jerarquico)
table(clus_jerarquico$clusterJerar)
```

De esta manera podemos ver que hay diferencias entre los grupos. Por ejemplo el grupo 1 está formado por clientes más jovenes, mientras que el grupo 4 está formado por clientes más veteranos con mucha antigüedad en la compañía. El grupo 3, por su parte, incluye a todos los siniestrados.

******
## Representación gráfica de variables por cluster  
******

Se puede actualizar el gráfico presentado previamente que relaciona la fidelidad con la experiencia incluyendo al asignación de clusters.

```{r,eval=TRUE,echo=TRUE}
plot(clus_jerarquico[c("ant_comp","ant_perm")], xlab="Fidelidad", ylab="Experiencia",col=clus_jerarquico$clusterJerar) 
title(main="Nube de puntos agrupados", col.main="blue", font.main=1)
```

El grupo 1 es el color negro, el grupo 2 es el color rojo, el grupo 3 es el color verde y el grupo 4 es el color azul.

```{r,eval=TRUE,echo=TRUE}
palette()[1:NumCluster]
```

******
# Agrupación no jerárquica: Algoritmo kmeans
******


******
## Uso de la función kmeans() para la formación de cluster (grupos o perfiles de individuos) 
******
Los algoritmos de segmentación no supervisados, como es el kmeans(), requieren que el analista determine cuál es el número de clústers (grupos) a formar, de hecho, la función kmeans() incorpora como parámetro el número de clústers (centers=).

Para seleccionar el número de grupos podemos utilizar criterios subjetivos o criterios objetivos. Los criterios subjetivos se basan en la visualización de los resultados para determinar el número de clústers más apropiado o en la simple experiencia. A continuación, utilizamos la función kmeans() para formar 3 grupos de individuos y visualizamos algunos resultados como son: los centros de grupos (centers), la suma de cuadrados totales (totss), las sumas de cuadrados dentro de cada grupo y para todos de forma conjunta (withinss y tot.withinss) y la suma de cuadrados entre grupos (betweenss). 

```{r,eval=TRUE,echo=TRUE}
set.seed(123)
modelo_k3<-kmeans(clus_norm,centers=3)
modelo_k3$centers
modelo_k3$totss
modelo_k3$withinss
modelo_k3$tot.withinss
modelo_k3$betweenss
```

******
## Elección del número de clústers   
******
Para la selección del número de clústers también existen criterios objetivos los cuales están basados en la optimización de un criterio de ajuste.

Los criterios de ajustes en el kmeans() se basan en los conceptos de sumas de cuadrados entre grupos (betweens) y dentro de grupos (withins). Hay que tener en cuenta que la suma de cuadrados entre grupos (betweenss) más las sumas de cuadrados dentro de grupos (tot.withinss) nos proporciona la suma de cuadrados totales (tots). Recordad también que las sumas de cuadrados corresponden a los numeradores de las varianzas correspondientes. 

Una segmentación se considera 'óptima' cuando, para cada grupo, los individuos son lo más homgéneos posibles mientras que son más heterogeneos a los individuos del resto de grupos Dicha segmentación coincidirá con aquella que, teniendo un número de grupos razonable, posee una "suma de cuadrados entre grupos"(betweenss) suficientemente grande y, por tanto, una "suma de cuadrados dentro de grupos" (tot.withinss) suficientemente pequeña. Es decir, la varianza dentro de grupos debe ser reducida (individuos dentro de un mismo grupo tiene que ser similares) y la varianza entre grupos debe ser grande (individuos de distintos grupos tienen que ser distintos). También, tenemos que tener en cuenta que a medida que el número de grupos aumenta la suma de cuadrados entre aumenta y, por tanto, la suma de cuadrados dentro disminuye, por tanto, el analista ha de decidir cuando el aumento de la suma de cuadrados entre o, alternativamente, la disminución de la suma de cuadrados dentro no son lo suficientemente pronunciados. Por ejemplo, comparamos los resultados para los casos de formar 2 y 3 grupos.

```{r,eval=TRUE,echo=TRUE}
#Suma de cuadrados entre grupos
kmeans(clus_norm,2)$betweenss
kmeans(clus_norm,3)$betweenss

#Suma de cuadrados dentro grupos
kmeans(clus_norm,2)$tot.withinss 
kmeans(clus_norm,3)$tot.withinss

#Suma de cuadrados total
kmeans(clus_norm,2)$totss 
kmeans(clus_norm,3)$totss
```

A continuación, definimos el modo de obtener un gráfico que nos represente la suma de cuadrados entre grupos en función del número de grupos.

```{r,eval=TRUE,echo=TRUE}
set.seed(123)
bss <- kmeans(clus_norm,centers=1)$betweenss
 for (i in 2:10) bss[i] <- kmeans(clus_norm,centers=i)$betweenss

plot(1:10, bss, type="l", xlab="Número de grupos",ylab="Sumas de cuadrados entre grupos")
```

******
## Asignación de los clusters   
******

Para mantener el conjunto de datos original lo primero que hacemos es crear una copia del conjunto de datos

```{r,eval=TRUE,echo=TRUE}
clus_kmeans=clus
```

Una vez elegido el número de clusters, se puede asignar a cada elemento el cluster asignado. En este caso vamos a elegir 5 clusters.

```{r,eval=TRUE,echo=TRUE}
NumCluster=5
set.seed(123)
Modelo=kmeans(clus_norm,NumCluster)
clus_kmeans$clusterKmeans= Modelo$cluster
head(clus_kmeans)
```

******
## Representación de los cluster  
******
Para poder interpretar los clusters es habitual representarlos mediante el valor medio de las variables en cada cluster.

```{r,eval=TRUE,echo=TRUE}
aggregate(.~clusterKmeans,FUN=mean, data=clus_kmeans)
table(clus_kmeans$clusterKmeans)
```

De esta manera podemos ver que hay diferencias entre los grupos. Por ejemplo el grupo 5 está formado por clientes más jovenes, mientras que el grupo 1 está formado por clientes  con mucha antigüedad en la compañía. El grupo 3, por su parte, incluye a todos los siniestrados y el grupo 4 recoge a asegurados con una antiguedad del vehículo superior.

******
## Representación gráfica de los clústers   
******

Se puede actualizar el gráfico presentado previamente que relaciona la fidelidad con la experiencia incluyendo la asignación de clusters.

```{r,eval=TRUE,echo=TRUE}
plot(clus_kmeans[c("ant_comp","ant_perm")], xlab="Fidelidad", ylab="Experiencia",col=clus_kmeans$clusterKmeans) 
title(main="Nube de puntos agrupados", col.main="blue", font.main=1)
```

El grupo 1 es el color negro, el grupo 2 es el color rojo, el grupo 3 es el color verde, el grupo 4 es el color azul y el grupo 5 es el de color cyan.

```{r,eval=TRUE,echo=TRUE}
palette()[1:NumCluster]
```

******
# Ejercicios PEC2
******

Para la realización de esta PEC vamos a utilizar el dataset Country data disponible en la página web de Kaggle (https://www.kaggle.com/datasets/rohan0301/unsupervised-learning-on-country-data). 

Comenzamos cargando el dataset, visualizando las primeras filas y viendo un resumen de los estadísticos más importantes para cada variabel

```{r,eval=TRUE,echo=TRUE}
countries = read.csv('E:/Análisis de datos/Fundamentos Data Science/PEC2/Country-data.csv', na.string = c("", "NA"))
head(countries)
summary(countries)
```

El dataset recoge información sociodemográfica y económica de 167 paises, en particular se muestran las siguientes variables:

* country: Name of the country
* child_mort: Death of children under 5 years of age per 1000 live births
* exports: Exports of goods and services per capita. Given as %age of the GDP per capita
* health: Total health spending per capita. Given as %age of GDP per capita
* imports: Imports of goods and services per capita. Given as %age of the GDP per capita
* income: Net income per person
* inflation: The measurement of the annual growth rate of the Total GDP
* life_expec: The average number of years a new born child would live if the current mortality patterns are to remain the same
* total_fer: The number of children that would be born to each woman if the current age-fertility rates remain the same.
* gdpp: The GDP per capita. Calculated as the Total GDP divided by the total population.

Se pide:

******
## Ejercicio 1
******

Analizad el dataset y elegid 5 variables (country no se incluye) para realizar un análisis no supervisado de clustering. Es conveniente argumentar el objetivo del análisis y justificar la selección de las 5 variables con dicho objetivo.

Describa los principales estadísticos de las 5 variables

******
## Respuesta 1
******

> Vamos a elegir las variables child_mort, health, income, life_expec, gdpp.
El objetivo es intentar relacionar la salud de los países con variables económicas, para saber cuanto influye la disponibilidad económica en la calidad y esperanza de vida. Hemos dejado "Exports" e "Imports" fuera ya que "income" y "gdpp" tal vez podrían tener un peso más concreto en la calidad de vida de las personas. "Inflation" podría ser también interesante, pero lo hemos tenido que descartar para poder limitar el estudio a 5 variables.

```{r,eval=TRUE,echo=TRUE,warning=FALSE, message=FALSE}

#Primero hacemos un subset con las variables elegidas y la columnas con el nombre de los países.

countries2<-countries[,c("child_mort","health","life_expec","income","gdpp")]

head(countries2)


#Ahora describiremos las variables:

summary(countries2)

```

> Respecto a la variable "child_mort", mortalidad infantil,  tenemos un mínimo de 2.60 muertes por 1000 niños por debajo de los 5 años y un Max de 208, con media de 38.27. Vemos que la diferencia entre mínimo y máximo se multiplica por 100.

>La variable "health", que es el gasto en salud en % por cápita, tiene un mínimo de 1.810, y un máximo de 17.90, con una media de 6.320. Vemos por tanto, que hay diferencias pero no tan grandes como en la variable "Child_mort".

>"life_expec", es decir, esperanza de vida, se sitúa entre un mínimo de 32.10 años y un máximo de 82.80, siendo la media de 70.56, por lo que vemos que la media se acerca más al máximo que al mínimo (similar en mediana). Esto nos hace suponer que hay pocos países con esperanza de vida tan bajas.

>"income", es el ingreso medio por persona. Aquí también hay grandes diferencias entre el mínimo (609) y el máximo (125000), la mediana se sitúa en 9960 y la media en 17145, por lo que parece que debe haber pocos países que se acerquen a los valores máximos. 

>"gdpp" es el producto interior bruto del país, que sirve como indicador económico general. Vuelve a haber diferencias grandes entre el mínimo (231) y el máximo (105000). La mediana (4660) y mediana (12964) son bastante diferentes pero también nos indican que no habrá muchos países cercanos al máximo.



```{r,eval=TRUE,echo=TRUE,warning=FALSE, message=FALSE}
#Creamos un gráfico para revisar la relación entre "health" y "child_mort"
plot(countries2[c("health","child_mort")], xlab="Gasto_Sanidad", ylab="Mortalidad_Infantil") 
title(main="GastoVsMortalidadInf", col.main="blue", font.main=1)

#Creamos un subset y gráfico de frecuencia para la columna "life_expec"
frecuencia<-table(countries2$life_expec)
frecuencia

barplot(frecuencia,xlab="Esperanza_Vida", ylab="Frecuencia")
title(main="Frecuencia Esperanza de Vida", col.main="blue", font.main=1)


```

******
## Ejercicio 2
******
Con los datos seleccionados en el ejercicio anterior, queremos realizar una clusterización considerando la técnica K-means. Entrene el modelo y justifique el número de clusters elegidos.

******
## Respuesta 2
******



```{r,eval=TRUE,echo=TRUE,warning=FALSE, message=FALSE}

#Para la normalización de las variables, primero copiamos copiamos el contenido en clus_countries_norm:

clus_countries_norm<-countries2


#Remplazamos las columnas de clus_countries_norm por las columnas de countries2 normalizadas:

 clus_countries_norm[,c("child_mort")] <- (countries2$child_mort-mean(countries2$child_mort))/sd(countries2$child_mort)
 clus_countries_norm[,c("health")] <- (countries2$health-mean(countries2$health))/sd(countries2$health)
 clus_countries_norm[,c("life_expec")] <- (countries2$life_expec-mean(countries2$life_expec))/sd(countries2$life_expec)
 clus_countries_norm[,c("income")] <- (countries2$income-mean(countries2$income))/sd(countries2$income)
 clus_countries_norm[,c("gdpp")] <- (countries2$gdpp-mean(countries2$gdpp))/sd(countries2$gdpp)

```

>Realizamos algunas representaciones gráficas para describir las variables normalizadas y comprobamos que la nube de puntos representada es igual a la original, lo único que cambia es la escala de los ejes.

```{r,eval=TRUE,echo=TRUE,warning=FALSE, message=FALSE}
#Normalizadas


plot(clus_countries_norm[c("life_expec","gdpp")], xlab="Esperanza_Vida", ylab="GDPP") 
title(main="LifeExpVSGDPP normalizado", col.main="blue", font.main=1)

#Originales


plot(countries2[c("life_expec","gdpp")], xlab="Esperanza_Vida", ylab="GDPP") 
title(main="LifeExpVSGDPP original", col.main="blue", font.main=1)
```


>Aplicamos la función k-means() para formar 3 grupos de países y visualizamos algunos resultados como son: los centros de grupos (centers), la suma de cuadrados totales (totss), las sumas de cuadrados dentro de cada grupo y para todos de forma conjunta (withinss y tot.withinss) y la suma de cuadrados entre grupos (betweenss). La elección del número de clusters es subjetiva ya que aun no hemos revisado ninguna técnica que nos haga estimar cual puede ser la mejor decisión.

```{r,eval=TRUE,echo=TRUE,warning=FALSE, message=FALSE}
set.seed(123)
modelo_k3<-kmeans(na.omit(clus_countries_norm),centers=3)

modelo_k3$centers
modelo_k3$totss
modelo_k3$withinss
modelo_k3$tot.withinss
modelo_k3$betweenss

```



******
## Ejercicio 3
******
Describa los clusters obtenidos mediante el modelo entrenado en el ejercicio previo.

******
## Respuesta 3
******


```{r,eval=TRUE,echo=TRUE,warning=FALSE, message=FALSE}

#Revisamos los centros de cada cluster con el siguiente código:

modelo_k3$centers


```



>  Cada fila corresponde a uno de los clusters creados para cada variable. Al ser valores estandarizados, los valores positivos están por encima de la media general y los negativos por debajo.

>En "child_mort" vemos diferencias grandes entre el cluster 1 y 3, teniendo los clusters 1 y 2 por debajo de la media, y el tercerro por encima. Las diferencias entre los clusters de "health" no son tan grandes.
Para "life_expec" vuelve a haber una diferencia grande entre cluster 1 y 3, donde el primero está por encima de la media, a una distancia similar el tercero, que está por debajo en su caso. 

>Para "income" y "gdpp vemos valores similares entre los clusters,  por encima de la media para el cluster 1, y valores negativos para los clusters 2 y 3.

```{r,eval=TRUE,echo=TRUE,warning=FALSE, message=FALSE}

#Además con withinss podemos averiguar cómo es de compacto cada cluster. Necesitaríamos que este valor fuera lo más bajo posible:

modelo_k3$withinss

```

>Son resultados bastante altos, el cluster 3 es el más compacto. 



******
## Ejercicio 4
******

Existen diferentes técnicas para la estimación del número óptimo de clusters que pueden proponernos una cantidad diferente. 

En este ejercicio vamos a analizar y comparar modelos entrenados con diferentes números de cluster para ver su similitudes y diferencias.

Se deberán entrenar 4 modelos k-means. comenzamos con un modelo con k=2 clusters para el que se tendrá que mostrar la distribución de elementos entre los dos clusters.

Seguiremos con k=3 mostrando la distribución de los elementos en los tres clusters y comparando con el modelo entrenado para k=2 respondiendo a las siguiente pregunta:

¿Hay alguna relación entre los 3 clusters de k=3 y los del modelo anterior k=2? en particular, ¿se mantiene un cluster y el otro se divide en dos clustesr o los nuevos 3 clusters no tienen ninguna relación aparente con los 2 anteriores?

Del mismo modo continuaremos entrenando modelos con k=4 clusters (que compararemos con k=3) y k=5 clusters (que comparemos con k=4) respondiendo a la misma pregunta:

¿Hay alguna relación entre los clusters del modelo entrenado y el que tiene un cluster menos? en particular, ¿se mantienen todos los clusters menos 1 y el otro se divide en dos clusters o los nuevos clusters no tienen ninguna relación aparente con los anteriores?

Finalmente, atendiendo a este ejercicio:

¿Qué cantidad de clusters propondrías como la adecuada? Razona la respuesta

******
## Respuesta 4
******
 

```{r,eval=TRUE,echo=TRUE,warning=FALSE, message=FALSE}

#Realizamos copia del set countries2
clus_countries_kmeans = countries2
  

#Realizamos primero el entrenamiento para 2 clusters:

NumCluster=2
set.seed(123)
Modelo=kmeans(clus_countries_norm,NumCluster)
clus_countries_kmeans$clusterKmeans= Modelo$cluster
head(clus_countries_kmeans)


#Representamos los clusters con el valor medio de las variables:

aggregate(.~clusterKmeans,FUN=mean, data=clus_countries_kmeans)
table(clus_countries_kmeans$clusterKmeans)


#Gráfico que relaciona la "life_expec" con "gdpp" incluyendo la asignación de clusters para los 2 clusters:


plot(clus_countries_kmeans[c("life_expec","gdpp")], xlab="Life Exp", ylab="Gdpp",col=clus_countries_kmeans$clusterKmeans) 
title(main="Nube de puntos agrupados", col.main="blue", font.main=1)


#Realizamos ahora el mismo entrenamiento para 3 clusters:

NumCluster=3
set.seed(123)
Modelo=kmeans(clus_countries_norm,NumCluster)
clus_countries_kmeans$clusterKmeans= Modelo$cluster
head(clus_countries_kmeans)

#Representamos los clusters con el valor medio de las variables de los 3 clusters:


aggregate(.~clusterKmeans,FUN=mean, data=clus_countries_kmeans)
table(clus_countries_kmeans$clusterKmeans)


#Gráfico que relaciona la "life_expec" con "gdpp" incluyendo la asignación de clusters para 3 clusters:


plot(clus_countries_kmeans[c("life_expec","gdpp")], xlab="Life Exp", ylab="Gdpp",col=clus_countries_kmeans$clusterKmeans) 
title(main="Nube de puntos agrupados", col.main="blue", font.main=1)
```

#¿Hay alguna relación entre los 3 clusters de k=3 y los del modelo anterior k=2? en particular, ¿se mantiene un cluster y el otro se divide en dos clustesr o los nuevos 3 clusters no tienen ninguna relación aparente con los 2 anteriores?


>Podemos ver como para k=2 tenemos lo siguiente:

```{r,eval=TRUE,echo=TRUE,warning=FALSE, message=FALSE}
#   1   2 
#  39 128 
``` 

>Para k=3 tenemos:

```{r,eval=TRUE,echo=TRUE,warning=FALSE, message=FALSE}
#  1  2  3 
# 31 90 46
```

>Como vemos el primer custer solo pierde 8 elementos, mientras que el cluster 2 es el que parece que se divide para crear el cluster 3.
Si revisamos las representaciones gráficas para cada uno de los k, se aprecia también esa divisón, donde vemos que el cluster 1 no ha cambiado demasiado (el cluster 1 es el color negro).

```{r,eval=TRUE,echo=TRUE,warning=FALSE, message=FALSE}

#Seguimos con el entrenamiento para 4 clusters:

NumCluster=4
set.seed(123)
Modelo=kmeans(clus_countries_norm,NumCluster)
clus_countries_kmeans$clusterKmeans= Modelo$cluster
head(clus_countries_kmeans)

#Representamos los clusters con el valor medio de las variables:

aggregate(.~clusterKmeans,FUN=mean, data=clus_countries_kmeans)
table(clus_countries_kmeans$clusterKmeans)


#Gráfico que relaciona la "life_expec" con "gdpp" incluyendo la asignación de clusters para los 4 clusters:

plot(clus_countries_kmeans[c("life_expec","gdpp")], xlab="Life Exp", ylab="Gdpp",col=clus_countries_kmeans$clusterKmeans) 
title(main="Nube de puntos agrupados", col.main="blue", font.main=1)
```

>Para k=3 tenemos:

```{r,eval=TRUE,echo=TRUE,warning=FALSE, message=FALSE}
#  1  2  3 
# 31 90 46
```
 
>Mientras que para el nuevo k=4 tenemos:
   
```{r,eval=TRUE,echo=TRUE,warning=FALSE, message=FALSE}
# 1  2  3  4 
#25 41 42 59
```

>Vemos que en este caso el cluster 1 ha perdido alguno de sus elementos, pero en este caso el cluster 2 (rojo) es el que más elementos pierde, ya que se divide en dos prácticamente. El cluster 3 mantiene bastantes de sus elementos, y el nuevo cluster 4 aparece con más elementos que el resto.
Parece, después de revisar la representación gráfica de k=4, que los clusters 2 y 3 (en rojo y azul) empiezan a estar mezclados entre ellos. 

```{r,eval=TRUE,echo=TRUE,warning=FALSE, message=FALSE}

#Seguimos con el entrenamiento para 5 clusters:
NumCluster=5
set.seed(123)
Modelo=kmeans(clus_countries_norm,NumCluster)
clus_countries_kmeans$clusterKmeans= Modelo$cluster
head(clus_countries_kmeans)

#Representamos los clusters con el valor medio de las variables:

aggregate(.~clusterKmeans,FUN=mean, data=clus_countries_kmeans)
table(clus_countries_kmeans$clusterKmeans)


#Gráfico que relaciona la "life_expec" con "gdpp" incluyendo la asignación de clusters para los 5 clusters:

plot(clus_countries_kmeans[c("life_expec","gdpp")], xlab="Life Exp", ylab="Gdpp",col=clus_countries_kmeans$clusterKmeans) 
title(main="Nube de puntos agrupados", col.main="blue", font.main=1)
```

>Para k=4 teníamos:

```{r,eval=TRUE,echo=TRUE,warning=FALSE, message=FALSE}
# 1  2  3  4 
#25 41 42 59
```

>Ahora con k=5:

```{r,eval=TRUE,echo=TRUE,warning=FALSE, message=FALSE}
# 1  2  3  4  5 
#22 52 37 50  6
```

>Se aprecia una pequeña pérdida en el cluster 1, pero llama la atención que el cluster 2 haya aumentado, lo que no había sido la norma hasta ahora. El cluster 3 y el 4 tienen pérdidas de elementos, aunque no muy grandes, y el cluster 5 se ha creado con solo 6 elementos.

>En la representación gráfica se aprecia la pérdida de elementos en el cluster 1 en favor del 5. 

```{r,eval=TRUE,echo=TRUE,warning=FALSE, message=FALSE}
palette()[1:NumCluster]
palette()[2:NumCluster]
palette()[3:NumCluster]
palette()[4:NumCluster]
palette()[5:NumCluster]
```


#¿Qué cantidad de clusters propondrías como la adecuada? Razona la respuesta
>Después de ver la distribución de elementos entre los distintos clusters para cada K, parece más adecuado k=4, ya que la distribución entre los elementos de los clusters parece más homogenea entre ellos. Los clusters 1 y 5 tienen una cantidad de elementos más alejada a 2 y 3, pero en el resto de entrenamientos las diferencias entre clusters son mayores.
>El único problema será que con k=4 parece que algunos elementos de clusters están más mezclados entre ellos (hay más elementos que parecen cercanos a otros clusters), lo que no está tan marcado con k=3.
>Por eso, la decisión estaría entre k=3 y k=4 dependiendo en si le dieramos más importancia a tener una cantidad más homogénea de elementos entre clusters o si preferiríamos evitar esa "mezcla" de elementos.

>Para k=4 teníamos:

```{r,eval=TRUE,echo=TRUE,warning=FALSE, message=FALSE}
# 1  2  3  4 
#25 41 42 59 
```

******
## Ejercicio 5
******
En el apartado de Manipulación y representación de variables se ha realizado un análisis gráfico de la relación entre el la antiguedad de la compañía y la antiguedad en el permiso de conducir 

Nos gustaria realizar un análisis similar entre dos de las variables elegidas. ¿Existe alguna relación?. Muestre la representación gráfica y razone la respuesta.

La seleccion de las 2 variables es libre pero se recomienda que se trate de buscar variables que puedan tener algun tipo de relación.

Adicionalmente se pide que se vuelvan a representar estas variables incluyendo el cluster en el color del elemento, es decir cada elemento representado deberá colorearse en función del cluster asociado para que todos los elementos del mismo cluster estén representados con el mismo color. COn este segundo gráfico. ¿Se ven nuevas relaciones entre las variables y los clusters? Descríbalas.

******
## Respuesta 5
******

> Elegiremos las variables "health","child_mort" con 3 clusters, para averiguar si hay relación entre un mayor gasto sanitario por cápita y la mortalidad infantil.

```{r,eval=TRUE,echo=TRUE,warning=FALSE, message=FALSE}

#Creamos otra copia del conjunto original:

clus_countries_jerarquico=countries2

#Establecemos la distancia euclidiana:

distan = dist(clus_countries_norm, method = "euclidean")

#Ahora agrupamos los elementos con hclust:
clus_countries_norm_Jerarquico = hclust(distan, method = "ward.D")


#En este caso vamos a elegir 3 clusters.

NumCluster=3
clus_countries_jerarquico$clusterJerarqCountr= cutree(clus_countries_norm_Jerarquico, k = NumCluster)
head(clus_countries_jerarquico,20)


#Representación gráfica de los clusters:
vcol <- c("green", "purple", "red")
plot(clus_countries_jerarquico[c("health","child_mort")], xlab="Gasto_Sanidad", ylab="Mort_Inf",col=vcol [clus_countries_jerarquico$clusterJerarqCountr]) 
title(main="Nube de puntos agrupados", col.main="blue", font.main=1)


#Para revisar que los colores sean los mismos en los 2 gráficos:
palette()[1:NumCluster]
palette()[2:NumCluster]
palette()[3:NumCluster]

#Se actualiza el gráfico presentado previamente que relaciona la "health" con "child_mort" incluyendo la asignación de clusters.

NumCluster=3
set.seed(123)
Modelo=kmeans(clus_countries_norm,NumCluster)
clus_countries_kmeans$clusterKmeans= Modelo$cluster
head(clus_countries_kmeans)


vcol2 <- c("red", "purple", "green")
plot(clus_countries_kmeans[c("health","child_mort")], xlab="Gasto_Sanidad", ylab="Mort_Inf",col=vcol2[clus_countries_kmeans$clusterKmeans]) 
title(main="Nube de puntos agrupados", col.main="blue", font.main=1)

#Para revisar que los colores sean los mismos en los 2 gráficos.
palette()[1:NumCluster]
palette()[2:NumCluster]
palette()[3:NumCluster]
```


>En general ambos gráficos nos indican que a un mayor inversión en sanidad suele haber una mayor esperanza de vida, aunque el patrón no es tan claro como el que habíamos visto antes de esperanza de vida y gdpp. El primer cluster en rojo parece incluir los países con menor mortalidad infantil, donde podemos ver que el gasto per cápita se parece acumularse en torno a 10.

>Al comparar los gráficos podemos ver que hay pequeñas diferencias entre ellos. El principal es que en la segunda representación para k-means el cluster 2 (lila) se amplia a costa sobretodo del cluster 3 y un poco del cluster 1. Aquí se ve más clara esa tendencía del cluster 1 a acumularse en torno al valor de gasto per capita 10.



******
## Ejercicio 6
******
En el apartado sobre la *_elección del número de clústers_* del algoritmo kmeans se muestra un gráfico que toma como variable referencia la suma de cuadrados entre '$betweenss' relacionado con el metodo del codo (elbow method)

Investigue otra técnica para seleccionar el número de clusters llamada Silhoutte (incluida en el paquete cluster). Describa dicha técnica. Además, utilizando está técnica indique el número óptimo de clusters para el conjunto de datos utilizado en los ejercicios previos. Por último, realice una representación gráfica que apoye la elección.

******
## Respuesta 6
******

>La función Silhouette también permite calcular cómo de similares son las observaciones del cluster al que están  asignadas. Una media alta en Silhouette indica un buen clustering. El número óptimo de clusters k es el que maximizará la media de Silhouette sobre un rango de valores posibles para k.
En este caso podemos incluir la función Silhouette dentro de otra función para su representación:

```{r,eval=TRUE,echo=TRUE,warning=FALSE, message=FALSE}
#Cargamos librerías
library (NbClust)
library (cluster)
library (clustertend)
library (factoextra)



#Usamos la función Silhouette
  silhouette_score <- function(k){
  km <- kmeans(clus_countries_norm, centers = k, nstart=25)
  ss <- silhouette(km$cluster, dist(clus_countries_norm))
  mean(ss[, 3])
}
k <- 2:10
avg_sil <- sapply(k, silhouette_score)
plot(k, type='b', avg_sil, xlab='Number of clusters', ylab='Average Silhouette Scores', frame=FALSE)



#Suma de cuadrados con betweenss:
set.seed(123)
bss <- kmeans(clus_countries_norm,centers=1)$betweenss
 for (i in 2:10) bss[i] <- kmeans(clus_countries_norm,centers=i)$betweenss

plot(1:10, bss, type="l", xlab="Número de grupos",ylab="Sumas de cuadrados entre grupos")
```


>Con la función Silhouette obtenemos un número óptimo de 2 clusters, sin embargo con betweens el óptimo estaría sobre el 3 e incluso 5 (aunque es cierto que a partir del 2 la pendiente del gráfico ya no es tan pronunciada)

> Tal vez establecer el número de clusters en 3 en este caso podría ser un término medio adecuado, ya que con 3 Silhouette aun está en un punto alto en cuanto a número óptimo de clusters, y con la suma de cuadrados la pendiente es bastante pronunciada hasta 3.

